{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f15cba8-c023-4f1c-8b4d-9f04735faae1",
   "metadata": {},
   "source": [
    "Steps for a one hidden layer neural network:\n",
    "- initialization / initialize()\n",
    "- forward propagation / forward_propagate()\n",
    "- cost computing / cost_compute\n",
    "- backward propagation / backward_propagate\n",
    "- weight update / weight_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b1707f-7470-4aa0-bcdb-5c289424c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8116484-071f-4c5d-ab6f-81afc73ceb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "def initialize(X, Y, units):\n",
    "    \n",
    "    n_x = X.shape\n",
    "    n_y = Y.shape\n",
    "    n_h = units\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x[0]) * 0.01 #low weights better for the gradient descent\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y[0], n_h)\n",
    "    b2 = np.zeros((n_y[0],1))\n",
    "    \n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2} #dico => :\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a094df-b93c-4dc9-a8ac-9e50f02f5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation\n",
    "def forward_propagate(X, parameters):\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\" : Z1, \"A1\" : A1, \"Z2\" : Z2, \"A2\" : A2}\n",
    "    \n",
    "    return A2, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd342a9b-8809-4314-a9f5-a00b2e0b2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost computation\n",
    "def cost_compute(Y_preds, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    abs_total_cost = np.dot(Y,np.log(Y_pred)) + np.dot(1-Y, np.log(1-Y_pred))\n",
    "    cost = -abs_total_cost/m #don't forget the -\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045b1403-e3a8-4aef-bb0f-ab9cfef42320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward propagation\n",
    "def backward_propagate(parameters, cache, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T)/m\n",
    "    db2 = np.sum(dZ2, axis =1, keepdims=True)/m\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1,2)))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims = True)/m\n",
    "    \n",
    "    grads = {\"dZ2\" : dZ2, \"dW2\" : dW2, \"db2\" : db2, \"dZ1\" : dZ1, \"dW1\" : dW1, \"db1\" : db1}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c05aae-f1d4-4f3a-9aca-ae99d636c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight update\n",
    "def weight_update(parameters, grads, learning_rate = 1.2):\n",
    "    \n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 -= learning_rate * dW1 #don't forget the learning rate\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078a8ee5-6472-407a-8df6-b654ba87339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "def neural_network(X, Y, n_h, num_iteration = 10000, print_cost = False):\n",
    "    \n",
    "    parameters = initialize(X, Y, units)\n",
    "    \n",
    "    for i in range(num_iteration):\n",
    "        \n",
    "        forward_propagate = forward_propagate(X,parameters)\n",
    "        A2 = forward_propagate[0]\n",
    "        cache = forward_propagate[1]\n",
    "        cost = cost_compute(A2, Y)\n",
    "        grads = backward_propagate(parameters, cache, X, Y)\n",
    "        parameters = weight_update(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faef01b1-8326-4576-a81d-827a274d67d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Prediction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m parameters \u001b[38;5;241m=\u001b[39m neural_network(\u001b[43mX\u001b[49m, Y, n_h\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m, print_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(X, parameters):\n\u001b[1;32m      4\u001b[0m     A2, cache \u001b[38;5;241m=\u001b[39m forward_propagate(X, parameters)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "parameters = neural_network(X, Y, n_h= 10, num_iteration = 10000, print_cost = False)\n",
    "def predict(X, parameters):\n",
    "    A2, cache = forward_propagate(X, parameters)\n",
    "    predictions = np.where(A2 > 0.5, 1, 0)\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
